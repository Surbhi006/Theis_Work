# -*- coding: utf-8 -*-
"""Q3_thesis_task.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v_TvPB5dRqdLFuxB87CqFTOi9eor6qbg
"""

pip install googletrans

pip install num2words

pip install pip install vaderSentiment

pip install emot

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

import json
import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import nltk
import re
from num2words import num2words
import string
from tqdm import tqdm
from googletrans import Translator
from emot.emo_unicode import UNICODE_EMO, EMOTICONS
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from wordcloud import WordCloud
from matplotlib import pyplot as plt
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from plotnine import *

def getData():
    with open ('/content/drive/My Drive/Yuvi_RIP.json', encoding="utf8") as f:
        d = json.load(f)
    data = {'a':(0, 0), 'b':(0,0), 'c':(0,0)}
    filter_data = []
    for i in range(len(d)):
        y=0
        r=0
        for j in range(len(d[i]['entities']['hashtags'])):
            if(d[i]['entities']['hashtags'][j]['text']=='RestInPeace'):
                r=1
            if(d[i]['entities']['hashtags'][j]['text']=='MissYouYuvi'):
                y=1
        if(y==1 or r==1):
            new_instance = {'id':d[i]['_id'], 'verified':d[i]['user']['verified'], 
                            'hashtags': d[i]['entities']['hashtags'], 
                            'loc': d[i]['user']['location'], 'created_at': d[i]['created_at'],'full_text':d[i]['full_text']}
            filter_data.append(new_instance)
        if(y==1 and r==1):
            a, b = data['c']
            if(d[i]['user']['verified']):
                b+=1
            a+=1
            data['c']=(a,b)
        elif(y==1):
            a, b = data['b']
            if(d[i]['user']['verified']):
                b+=1
            a+=1
            data['b']=(a,b)
        elif(r==1):
            a, b = data['a']
            if(d[i]['user']['verified']):
                b+=1
            a+=1
            data['a']=(a,b)
    print(data)
    return filter_data

def timeSeries(data):
    result = {'a':{}, 'b':{}, 'c':{}}
    it = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13',
          '14', '15', '16', '17', '18', '19', '20', '21', '22', '23']
    for i in range(len(it)):
        result['a'][it[i]]=0
        result['b'][it[i]]=0
        result['c'][it[i]]=0
    for i in range(len(data)):
        time = data[i]['created_at']
        time = time[11:13]
        y=0
        r=0
        for j in range(len(data[i]['hashtags'])):
            if(data[i]['hashtags'][j]['text']=='RestInPeace'):
                r=1
            if(data[i]['hashtags'][j]['text']=='MissYouYuvi'):
                y=1
        if(r==1 and y==1):
            result['c'][time]+=1
        elif(r==1):
            result['a'][time]+=1
        elif(y==1):
            result['b'][time]+=1
    X = {'a':[], 'b':[], 'c':[]}
    Y = {'a':[], 'b':[], 'c':[]}
    for i in result:
        for j in sorted(result[i].keys()):
            X[i].append(j)
            Y[i].append(result[i][j])
    print(X)
    print(Y)
    return X, Y

def generate_topics(filter_data):
  topicA=[]
  topicB=[]
  topicC=[]
  for i in range(len(filter_data)):
    y=0
    r=0
    tweet = filter_data[i]['full_text']
    for j in range(len(filter_data[i]['hashtags'])):
        if(filter_data[i]['hashtags'][j]['text']=='RestInPeace'):
            r=1
        if(filter_data[i]['hashtags'][j]['text']=='MissYouYuvi'):
            y=1
    if(r==1 and y==1):
      
      topicC.append(tweet) #storing tweets of both
        
    elif(r==1):
        topicA.append(tweet) #storing tweets of RIP
    elif(y==1):
        topicB.append(tweet) #storing tweets of Yuvi
  return topicA,topicB,topicC

filter_data = getData()
X, Y = timeSeries(filter_data)
topicA,topicB,topicC = generate_topics(filter_data)

print(len(topicA))

"""Preprocessing"""

#Remove urls from the tweets
def remove_urls(text): 
    text = ''.join(text)
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)

#removes users from the tweets
def remove_users(text):
    text = re.sub('(RT\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)
    text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)
    return text

#Convert emojis to word
from emot.emo_unicode import UNICODE_EMO, EMOTICONS
def convert_emojis(text):
    for emot in UNICODE_EMO:
        text = text.replace(emot, "_".join(UNICODE_EMO[emot].replace(",","").replace(":","").split()))
    return text

#Convert emoticons to word
def convert_emoticons(text):
    for emot in EMOTICONS:
        text = re.sub(u'('+emot+')', "_".join(EMOTICONS[emot].replace(",","").split()), text)
    return text

#Translates all languages to english
def convert_language(text):
  translator = Translator()
  translations = translator.translate(text,dest='en')
  return translations.text

#Preprocessing on tweets
def preprocess_data(text):
    token_list=[] 
    stop_words = set(stopwords.words('english'))
    ps = nltk.PorterStemmer()
    lemmatizer = WordNetLemmatizer() 
    text = remove_urls(text)
    text = remove_users(text)
    text = convert_language(text)
    text = convert_emoticons(text)
    text = text.lower()
    text = convert_emojis(text)
    
    text = word_tokenize(text) #generate tokens
    punc = string.punctuation
    tokens = [i for i in text if not i in (stop_words and punc)] #remove stop words
    for word in tokens:
        w=word.translate(str.maketrans('','',string.punctuation)) #remove puntuations
        if w!= '' and w not in stop_words:
          w=lemmatizer.lemmatize(w) #lemmatize the strings
          if w.isnumeric():
              w = re.sub('([0-9]+)', '', w) #remove digits
              token_list.append(w)
          else:
              token_list.append(w)
    return token_list

def preprocessed_data(data):
  preL = []
  for i in tqdm(data):
    pre = preprocess_data(i)
    pre = ' '.join(pre)
    preL.append(pre)
  return preL

preA = preprocessed_data(topicA) #preprocessed List of RIP
preB = preprocessed_data(topicB) #preprocessed List of Yuvi
preC = preprocessed_data(topicC) #preprocessed List of both

print(len(preA))

"""LDA model"""

#Generate Vectors
def generate_vector(data):
  vectorizer = CountVectorizer(max_df=0.9, min_df=30, token_pattern='\w+|\$[\d\.]+|\S+')
  tf_class = vectorizer.fit_transform(data)
  analyze = vectorizer.build_analyzer()
  tf_feature_names = vectorizer.get_feature_names()
  tf_class = tf_class.toarray()
  feat_dict=vectorizer.vocabulary_.keys()
  return tf_class,tf_feature_names,feat_dict

tf_classA, tf_feature_namesA,feat_dictA = generate_vector(preA) #vector and features
tf_classB,tf_feature_namesB,feat_dictB = generate_vector(preB)
tf_classC,tf_feature_namesC,feat_dictC = generate_vector(preC)

tf_classA.shape
print(tf_feature_namesA)

#LDA model implementation take tf and number of topic words as input
def topic_model(tf_class,number_of_topics):
  model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)
  model.fit(tf_class)
  return model

def display_topics(model, feature_names, no_top_words):
    topic_dict = {}
    for topic_idx, topic in enumerate(model.components_):
        topic_dict["Topic %d words" % (topic_idx)]= ['{}'.format(feature_names[i])
                        for i in topic.argsort()[:-no_top_words - 1:-1]]
        topic_dict["Topic %d weights" % (topic_idx)]= ['{:.1f}'.format(topic[i])
                        for i in topic.argsort()[:-no_top_words - 1:-1]]
    return pd.DataFrame(topic_dict)

topic_dict = display_topics(modelA,tf_feature_namesA,no_top_words)
print(topic_dict)

"""#RESTINPEACE topic model"""

modelA = topic_model(tf_classA,10)
no_top_words = 10
display_topics(modelA, tf_feature_namesA, no_top_words)

all_words = ' '.join([text for text in topic_dict['Topic 5 words']])
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""#**MISSYOUYUVI** topic model"""

modelB = topic_model(tf_classB,10)
no_top_words = 10
display_topics(modelB, tf_feature_namesB, no_top_words)

topic_dict = display_topics(modelB, tf_feature_namesB, no_top_words)

all_words = ' '.join([text for text in topic_dict['Topic 1 words'] ])
from wordcloud import WordCloud
from matplotlib import pyplot as plt
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""#BOTH topic model"""

modelC = topic_model(tf_classC,10)
no_top_words = 10
display_topics(modelC, tf_feature_namesC, no_top_words)

all_words = ' '.join([text for text in preC])
from wordcloud import WordCloud
from matplotlib import pyplot as plt
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""**SentimentAnalysis**


---



---


(Used Vader sentiment tool to classify them in positive,negative and neutral)
"""

def sentiment_info(data):
  analyser = SentimentIntensityAnalyzer()
  pos_count=0
  neu_count=0
  neg_count=0
  sentiment={}
  for tweet in data:
    vs =analyser.polarity_scores(tweet)
    if vs['compound']>= 0.05:
       pos_count+=1
       sentiment[tweet]=['positive',vs['compound'],vs['neg'],vs['neu'],vs['pos']]
    elif vs['compound']<= -0.05:
      neg_count+=1
      sentiment[tweet]=['negative',vs['compound'],vs['neg'],vs['neu'],vs['pos']]
    else:
      sentiment[tweet]=['neutral',vs['compound'],vs['neg'],vs['neu'],vs['pos']]
      neu_count+=1
  print("Positive: ",pos_count)
  print("Negative: ",neg_count)
  print("Neutral: ",neu_count)
  return sentiment

sentimentA = sentiment_info((preA))

sentimentB = sentiment_info(preB)

sentimentC = sentiment_info(preC)

import matplotlib.pyplot as plt
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.set_ylabel('Tweets')
ax.set_xlabel('Tweets from Both Class')
langs = ['Positive', 'Negative', 'Neutral']
students = [80, 71, 118]
ax.bar(langs,students)
plt.show()

#To find prominent word from tweets
def pro_word(text, score):
  text = text.split(' ')
  best = None
  gs = 10
  for word in text:
    sc=analyser.polarity_scores(word)
    if abs(sc['compound']-score)<gs:
      gs = abs(sc['compound']-score)
      best = word
  return best

s={}
thres=1
def sinsert(val):
  for i in s:
    if abs(s[i]-val)<thres:
      return False
  return True

#To generate dataframe for ggplot or chatterplot
from operator import itemgetter
s = {}
def generate_dataframe(sentimentA):
  dp=[]
  i=0
  for key in sentimentA:
    word = pro_word(key, float(sentimentA[key][1]))
    temp=(i,key,sentimentA[key][0],sentimentA[key][1],sentimentA[key][2],sentimentA[key][3], sentimentA[key][4], word)
    if temp not in dp:
      dp.append(temp)
    i+=1
  dp = sorted(dp, key = itemgetter(3))
  new_dp = dp[:10]
  for i in range(len(dp)-1, len(dp)-20, -1):
    new_dp.append(dp[i])
  df = pd.DataFrame(new_dp,columns=['ID','Tweet','class','compound score','neg','neu','pos', 'word'])
  print(len(new_dp))
  return df

dfA = generate_dataframe(sentimentA) #SentimentAnalysis class of A-->RIP
dfB = generate_dataframe(sentimentB) #SentimentAnalysis class of B-->MissYouYuvi
dfC = generate_dataframe(sentimentC) ##SentimentAnalysis class of C-->both

dfA

"""***[Chatter Plot of RestInPeace](https:// [link text](https:// [link text](https:// [link text](https://))))***

> Indented block

> Indented block

> Indented block
"""

ggplot(dfA, aes("compound score","ID")) +\
geom_text(aes(label="word",alpha=0,colour="compound score"),size=7,va="bottom", ha="right",nudge_x=7,nudge_y=7)+\
scale_x_log10() +\
scale_color_gradient(low='green', high='red',trans='log10',guide = guide_colourbar(direction = "horizontal",title_position="top"))+\
theme(legend_position="none",panel_grid_major = element_line(colour = "whitesmoke"))+\
theme_minimal()+\
labs(y = "ID", x = "compound score") +\
ggtitle("words vs emotions")

"""ChatterPlot of MissYouYuvi"""

ggplot(dfB, aes("compound score","ID")) +\
geom_text(aes(label="word",alpha=0,colour="compound score"),size=7,va="bottom", ha="right")+\
scale_x_log10() +\
scale_color_gradient(low='green', high='red',trans='log10',guide = guide_colourbar(direction = "horizontal",title_position="top"))+\
theme(legend_position="none",panel_grid_major = element_line(colour = "whitesmoke"))+\
theme_minimal()+\
labs(y = "ID", x = "compound score") +\
ggtitle("words vs emotions")

"""ChatterPlot of Both"""

ggplot(dfC, aes("compound score","ID")) +\
geom_text(aes(label="word",alpha=0,colour="compound score"),size=7,va="bottom", ha="right")+\
scale_x_log10() +\
scale_color_gradient(low='green', high='red',trans='log10',guide = guide_colourbar(direction = "horizontal",title_position="top"))+\
scale_size_continuous(limits=(0, 250),breaks=range(0, 250, 5),expand=(0, 0))+\
theme(legend_position="none",panel_grid_major = element_line(colour = "whitesmoke"))+\
theme_minimal()+\
labs(y = "ID", x = "compound score") +\
ggtitle("words vs emotions")

def generate_dataframe(sentiment):
  dp=[]
  i=0
  for key in sentimentA:
    word = pro_word(key, float(sentimentA[key][1]))
    if 'restinpeace' in key:
      temp=(i,key,sentimentA[key][0],sentimentA[key][1],sentimentA[key][2],sentimentA[key][3], sentimentA[key][4], word)
    if temp not in dp:
      dp.append(temp)
    i+=1
  df = pd.DataFrame(dp,columns=['ID','Tweet','class','compound score','neg','neu','pos', 'word'])
  return df