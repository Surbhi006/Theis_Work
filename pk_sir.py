# -*- coding: utf-8 -*-
"""PK_Sir.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IZeMvelc02FFx5Rj_Rsu1rurrIMZ-DIc
"""

import json

#Generating number of tweets under a particular class
#A-->RIP
#B-->MissyouYuvi
#C-->Both
def getData():
    with open ('/content/drive/My Drive/Yuvi_RIP.json', encoding="utf8") as f:
        d = json.load(f)
        length = len(d)
        print(len(d))
    data = {'a':(0, 0), 'b':(0,0), 'c':(0,0)}
    filter_data = []
    for i in range(len(d)):
        y=0
        r=0
        for j in range(len(d[i]['entities']['hashtags'])):
            if(d[i]['entities']['hashtags'][j]['text']=='RestInPeace'):
                r=1
            if(d[i]['entities']['hashtags'][j]['text']=='MissYouYuvi'):
                y=1
        if(y==1 or r==1):
            new_instance = {'id':d[i]['_id'], 'verified':d[i]['user']['verified'], 
                            'hashtags': d[i]['entities']['hashtags'], 'metadata':d[i]['metadata'], 
                            'loc': d[i]['user']['location'], 'created_at': d[i]['created_at'],'full_text':d[i]['full_text']}
            filter_data.append(new_instance) #Storing required Data
        if(y==1 and r==1):
            a, b = data['c']
            if(d[i]['user']['verified']):
                b+=1
            a+=1
            data['c']=(a,b)
        elif(y==1):
            a, b = data['b']
            if(d[i]['user']['verified']):
                b+=1
            a+=1
            data['b']=(a,b)
        elif(r==1):
            a, b = data['a']
            if(d[i]['user']['verified']):
                b+=1
            a+=1
            data['a']=(a,b)
    print(data)
    return filter_data

#For plotting tweets in an hour
def timeSeries(data):
    result = {'a':{}, 'b':{}, 'c':{}}
    it = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13',
          '14', '15', '16', '17', '18', '19', '20', '21', '22', '23']
    for i in range(len(it)):
        result['a'][it[i]]=0
        result['b'][it[i]]=0
        result['c'][it[i]]=0
    for i in range(len(data)):
        time = data[i]['created_at']
        time = time[11:13]
        y=0
        r=0
        for j in range(len(data[i]['hashtags'])):
            if(data[i]['hashtags'][j]['text']=='RestInPeace'):
                r=1
            if(data[i]['hashtags'][j]['text']=='MissYouYuvi'):
                y=1
        if(r==1 and y==1):
            result['c'][time]+=1
        elif(r==1):
            result['a'][time]+=1
        elif(y==1):
            result['b'][time]+=1
    X = {'a':[], 'b':[], 'c':[]}
    Y = {'a':[], 'b':[], 'c':[]}
    for i in result:
        for j in sorted(result[i].keys()):
            X[i].append(j)
            Y[i].append(result[i][j])
    print(X)
    print(Y)
    return X, Y

#for plotting tweets on a particular date
def dateSeries(data):
  count = 0
  date_data={}
  c1 =0
  c2 = 0
  c3 = 0
  for i in range(len(data)):
    date = (data[i]['created_at'])
    if date not in date_data:
      #print(date)
      date_data[date]={'a':0,'b':0,'c':0}
      class_name = filter_data[i]['hashtags']
      #print(class_name)
      for j in range(len(class_name)):
        c = class_name[j]['text']
        if(c=='MissYouYuvi'):
          c1+=1
          date_data[date]['a']=c1
        if(c=='RestInPeace'):
          c2+=1
          date_data[date]['b'] =c2
        if(c=='RestInPeace'and c=='MissYouYuvi'):
          c3+=1
          date_data[date]['c']=c3
  return date_data

filter_data = getData()
#print(len(filter_data))
X, Y = timeSeries(filter_data)
date_data = dateSeries(filter_data)

#RESTINPEACE
import matplotlib.pyplot as plt
plt.bar(X['a'],Y['a'])

#MissyouYuvi
plt.bar(X['b'],Y['b'])

#Both
plt.bar(X['c'],Y['c'])

#COMparison in all
plt.plot(X['a'], Y['a'], label='class A')
plt.plot(X['b'], Y['b'], label='class B')
plt.plot(X['c'], Y['c'], label='class C')


plt.title('Time-series plot')
plt.ylabel('Hashtags')
plt.xlabel('Time in hours')
plt.legend()
plt.grid()
plt.show()

print(topic_dict['a'])

for i in topic_dict:
  count = len(topic_dict[i])
  print(count)

import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import nltk
import re
from num2words import num2words
import string

nltk.download('punkt')

pip install num2words

def remove_urls(text):
    text = ''.join(text)
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)

def remove_users(text):
    text = re.sub('(RT\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text) # remove retweet
    text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text) # remove tweeted at
    return text

pip install emot

#Convert emoticons to word
from emot.emo_unicode import UNICODE_EMO, EMOTICONS
def convert_emojis(text):
    for emot in UNICODE_EMO:
        text = text.replace(emot, "_".join(UNICODE_EMO[emot].replace(",","").replace(":","").split()))
    return text

def convert_emoticons(text):
    for emot in EMOTICONS:
        text = re.sub(u'('+emot+')', "_".join(EMOTICONS[emot].replace(",","").split()), text)
    return text

def preprocess_data(text,bigrams=False):
    token_list=[] 
    stop_words = set(stopwords.words('english'))
    ps = nltk.PorterStemmer()
    text = remove_urls(text)
    text = remove_users(text)
    text = convert_emoticons(text)
    text = text.lower()
    text = convert_emojis(text)
    
    text = word_tokenize(text)
    punc = string.punctuation
    tokens = [i for i in text if not i in (stop_words and punc)]
    for word in tokens:
        w=word.translate(str.maketrans('','',string.punctuation))
        if w!= '' and w not in stop_words:
          w=ps.stem(w)
          if w.isnumeric():
              w = re.sub('([0-9]+)', '', w)
              token_list.append(w)
          else:
              token_list.append(w)
    if bigrams:
        token_list = token_list+[token_list[i]+'_'+token_list[i+1]
                                            for i in range(len(token_list)-1)]
    return token_list

text = 'ðŸ˜­ðŸ˜­\n\n#RestInPeace https://t.co/N3Qm59QSjz'
ans = preprocess_data(text)
print(ans)

#tf of term in a tweet for a particular class
def TF():
  tf = {'a':{}, 'b':{}, 'c':{}}
  i = 0
  for c in topic_dict:
      for tweet in topic_dict[c]:
          data = preprocess_data(tweet)
          tf[c][str(i)] = {}
          for item in data:
              if(item in tf[c][str(i)]):
                  tf[c][str(i)][item]+=1
              else:
                  tf[c][str(i)][item]=1
          i+=1
  return tf

print(tf)

def generate_vocab():
  vocab =[]
  for c in tf:
    for n in tf[c]:
      for word in tf[c][n]:
        if word not in vocab:
          vocab.append(word)
  unique_words = set(vocab)
  return unique_words

import math
def generateIDF():

  idf = {'a':{},'b':{},'c':{}}
  j=0
  for c in tf:
    N = len(tf[c])
    for n in tf[c]: 
      for word in tf[c][n]:
          val = 0
          for doc in tf[c]:
            if word in tf[c][doc]:
              val+=1
          idf[c][str(word)]=math.log((N/val),10)
  return idf